{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkHtfLRzUs78"
      },
      "outputs": [],
      "source": [
        "# One-cell launcher: wrote app, started Streamlit, opened Cloudflared tunnel\n",
        "import os, sys, subprocess, time, re, urllib.request, stat\n",
        "\n",
        "# installed deps\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"streamlit\", \"plotly\", \"scikit-learn\"])\n",
        "\n",
        "# ensured cloudflared binary (no signup needed)\n",
        "CF_PATH = \"/content/cloudflared\" # Changed path to /content/\n",
        "if not os.path.exists(CF_PATH):\n",
        "    url = \"https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\"\n",
        "    urllib.request.urlretrieve(url, CF_PATH)\n",
        "    # Add execute permissions for the owner\n",
        "    os.chmod(CF_PATH, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR |\n",
        "                      stat.S_IRGRP | stat.S_IXGRP |\n",
        "                      stat.S_IROTH | stat.S_IXOTH)\n",
        "\n",
        "# wrote Streamlit app with renamed sections; removed \"Next steps\"; comments in past tense\n",
        "APP_PATH = \"/content/EDA_in_streamlit_ORD.py\"\n",
        "APP_CODE = r'''\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, roc_curve\n",
        ")\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "st.set_page_config(page_title=\"ORD TSA — EDA & ML\", layout=\"wide\")\n",
        "st.title(\"ORD TSA Throughput — EDA & Predictive Modelling\")\n",
        "\n",
        "# sidebar navigation\n",
        "section = st.sidebar.radio(\n",
        "    \"Section\",\n",
        "    [\n",
        "        \"1. Project Introduction\",\n",
        "        \"2. Data Overview\",\n",
        "        \"3. Exploratory Data Analysis\",\n",
        "        \"4. Feature Engineering\",\n",
        "        \"5. Predictive Modeling\",\n",
        "        \"6. Model Selection & Tuning\",\n",
        "        \"7. Results & Conclusion\"\n",
        "    ],\n",
        "    index=0\n",
        ")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload TSA CSV (TsaThroughput.ORD.csv)\", type=[\"csv\"])\n",
        "granularity = st.sidebar.radio(\"Granularity\", [\"Daily\", \"Hourly\"], index=0)\n",
        "plot_theme = st.sidebar.selectbox(\"Plot theme\", [\"plotly\", \"plotly_white\", \"plotly_dark\"], index=0)\n",
        "\n",
        "# loaded CSV (or small synthetic fallback), parsed date/hour, engineered basic features\n",
        "@st.cache_data(show_spinner=False)\n",
        "def load_ord_csv(file_like):\n",
        "    info = {\"source\": None, \"message\": \"\"}\n",
        "    if file_like is not None:\n",
        "        df_raw = pd.read_csv(file_like)\n",
        "        info[\"source\"] = \"uploaded\"\n",
        "    elif os.path.exists(\"TsaThroughput.ORD.csv\"):\n",
        "        df_raw = pd.read_csv(\"TsaThroughput.ORD.csv\")\n",
        "        info[\"source\"] = \"default_file\"\n",
        "    else:\n",
        "        days = 150\n",
        "        idx = pd.date_range(end=pd.Timestamp.today().normalize(), periods=days, freq=\"D\")\n",
        "        base = 42000 + 4000*np.sin(2*np.pi*idx.dayofweek/7)\n",
        "        noise = np.random.normal(0, 2500, size=days)\n",
        "        daily = pd.DataFrame({\"date\": idx, \"pax\": np.maximum(1000, base + noise)})\n",
        "        df_raw = pd.DataFrame({\n",
        "            \"Date\": np.repeat(idx, 24),\n",
        "            \"Hour\": np.tile(pd.date_range(\"00:00\", \"23:00\", freq=\"1h\").strftime(\"%H:%M:%S\"), days),\n",
        "            \"ORD A\": np.repeat(daily[\"pax\"].values/24, 24)\n",
        "        })\n",
        "        info[\"source\"] = \"synthetic\"\n",
        "        info[\"message\"] = \"No CSV found; used a small synthetic sample. Upload the real CSV in the sidebar.\"\n",
        "\n",
        "    ord_cols = [c for c in df_raw.columns if isinstance(c, str) and c.startswith(\"ORD \")]\n",
        "    if not ord_cols:\n",
        "        ignore = {\"Date\",\"Hour\",\"date\",\"hour\"}\n",
        "        ord_cols = [c for c in df_raw.columns if c not in ignore and pd.api.types.is_numeric_dtype(df_raw[c])]\n",
        "    df = df_raw.copy()\n",
        "    df[\"pax\"] = df[ord_cols].sum(axis=1) if ord_cols else (df.iloc[:, -1] if len(df.columns) else 0)\n",
        "\n",
        "    if \"date\" in df.columns:\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    else:\n",
        "        df[\"date\"] = pd.to_datetime(df.get(\"Date\", pd.NaT), errors=\"coerce\")\n",
        "\n",
        "    if \"hour\" in df.columns and pd.api.types.is_numeric_dtype(df[\"hour\"]):\n",
        "        df[\"hour\"] = df[\"hour\"].astype(int)\n",
        "    elif \"Hour\" in df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df[\"Hour\"]):\n",
        "            df[\"hour\"] = df[\"Hour\"].astype(int)\n",
        "        else:\n",
        "            parsed = pd.to_datetime(df[\"Hour\"], errors=\"coerce\")\n",
        "            df[\"hour\"] = np.where(parsed.notna(), parsed.dt.hour, 0)\n",
        "    else:\n",
        "        df[\"hour\"] = 0\n",
        "\n",
        "    df[\"dow\"] = df[\"date\"].dt.dayofweek\n",
        "    df[\"day_of_week\"] = df[\"date\"].dt.day_name()\n",
        "    df[\"month\"] = df[\"date\"].dt.month\n",
        "    df[\"month_name\"] = df[\"date\"].dt.month_name()\n",
        "\n",
        "    daily = (\n",
        "        df.groupby(\"date\", as_index=False)[\"pax\"].sum()\n",
        "          .sort_values(\"date\").set_index(\"date\").asfreq(\"D\")\n",
        "    )\n",
        "    daily[\"pax\"] = daily[\"pax\"].interpolate(limit_direction=\"both\")\n",
        "    daily[\"dow\"] = daily.index.dayofweek\n",
        "    daily[\"day_of_week\"] = daily.index.day_name()\n",
        "    daily[\"month\"] = daily.index.month\n",
        "    daily[\"month_name\"] = daily.index.month_name()\n",
        "    daily[\"dom\"] = daily.index.day\n",
        "    daily[\"is_weekend\"] = (daily[\"dow\"] >= 5).astype(int)\n",
        "\n",
        "    for L in [1,7,14]:\n",
        "        daily[f\"lag_{L}\"] = daily[\"pax\"].shift(L)\n",
        "    for W in [7,14]:\n",
        "        daily[f\"rollmean_{W}\"] = daily[\"pax\"].shift(1).rolling(W).mean()\n",
        "\n",
        "    return df, daily, info\n",
        "\n",
        "df_hourly, df_daily, info = load_ord_csv(uploaded_file)\n",
        "if info[\"source\"] == \"synthetic\":\n",
        "    st.warning(info[\"message\"])\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-8, None))) * 100\n",
        "\n",
        "# built ML frames with time-wise split; prepared encoders/scalers\n",
        "@st.cache_data(show_spinner=False)\n",
        "def build_ml_frames(daily):\n",
        "    daily = daily.dropna().copy()\n",
        "    if len(daily) < 30:\n",
        "        return None\n",
        "    daily[\"dow_name\"] = daily.index.day_name()\n",
        "    daily[\"month_name\"] = daily.index.month_name()\n",
        "\n",
        "    y_raw = daily[\"pax\"]\n",
        "    y_reg = np.log1p(daily[\"pax\"])\n",
        "\n",
        "    num_cols = [c for c in [\"dow\",\"dom\",\"month\",\"is_weekend\",\"lag_1\",\"lag_7\",\"lag_14\",\"rollmean_7\",\"rollmean_14\"] if c in daily.columns]\n",
        "    cat_cols = [\"dow_name\",\"month_name\"]\n",
        "\n",
        "    X = daily[num_cols + cat_cols]\n",
        "    n = len(X); split = int(n * 0.7)\n",
        "    X_train = X.iloc[:split]; X_test = X.iloc[split:]\n",
        "    y_reg_train = y_reg.iloc[:split]; y_reg_test = y_reg.iloc[split:]\n",
        "    y_raw_train = y_raw.iloc[:split]; y_raw_test = y_raw.iloc[split:]\n",
        "\n",
        "    thresh = y_raw_train.quantile(0.75)\n",
        "    y_cls = (y_raw >= thresh).astype(int)\n",
        "    y_cls_train = y_cls.iloc[:split]; y_cls_test = y_cls.iloc[split:]\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", RobustScaler(), num_cols),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.0\n",
        "    )\n",
        "    X_train_pp = pre.fit_transform(X_train)\n",
        "    X_test_pp  = pre.transform(X_test)\n",
        "\n",
        "    cat_names = list(pre.named_transformers_[\"cat\"].get_feature_names_out(cat_cols)) if cat_cols else []\n",
        "    feat_names = num_cols + cat_names\n",
        "\n",
        "    return {\n",
        "        \"X_train\": X_train, \"X_test\": X_test,\n",
        "        \"X_train_pp\": X_train_pp, \"X_test_pp\": X_test_pp,\n",
        "        \"y_reg_train\": y_reg_train, \"y_reg_test\": y_reg_test,\n",
        "        \"y_raw_train\": y_raw_train, \"y_raw_test\": y_raw_test,\n",
        "        \"y_cls_train\": y_cls_train, \"y_cls_test\": y_cls_test,\n",
        "        \"pre\": pre, \"feat_names\": feat_names, \"thresh\": thresh,\n",
        "        \"num_cols\": num_cols, \"cat_cols\": cat_cols\n",
        "    }\n",
        "\n",
        "ml = build_ml_frames(df_daily)\n",
        "\n",
        "# sections\n",
        "if section == \"1. Project Introduction\":\n",
        "    st.subheader(\"Problem Statement\")\n",
        "    st.write(\"Forecast daily passenger throughput at ORD and flag high-surge days so operations could staff lanes and resources proactively.\")\n",
        "    st.subheader(\"Context & Use Cases\")\n",
        "    st.markdown(\n",
        "        \"- Why it mattered: unexpected surges created delays, SLA breaches, and overtime.\\n\"\n",
        "        \"- Who used it: checkpoint managers, schedulers, airline ops, duty managers.\\n\"\n",
        "        \"- Decisions supported: next-day staffing, lane configuration, airline comms.\"\n",
        "    )\n",
        "    st.subheader(\"Targets & Labelling\")\n",
        "    with st.expander(\"How the two targets were defined\", expanded=True):\n",
        "        st.markdown(\n",
        "            \"- Regression target: `log1p(pax)` where `pax` is total daily passengers.\\n\"\n",
        "            \"- Classification target (surge): 1 if the day's `pax` ≥ the 75th percentile of the training period; else 0.\"\n",
        "        )\n",
        "        if ml and \"thresh\" in ml:\n",
        "            st.info(f\"Training surge threshold ≈ {int(ml['thresh']):,} pax/day.\")\n",
        "    st.subheader(\"Assumptions\")\n",
        "    st.markdown(\n",
        "        \"- Hourly rows were summed to daily; short gaps were interpolated.\\n\"\n",
        "        \"- Seasonality was represented by calendar fields, lags, and rolling means.\\n\"\n",
        "        \"- External drivers (holidays/weather) were not included in this version.\"\n",
        "    )\n",
        "    st.subheader(\"Key Fields\")\n",
        "    st.markdown(\n",
        "        \"- date, pax\\n\"\n",
        "        \"- dow, dom, month, is_weekend\\n\"\n",
        "        \"- lag_1, lag_7, lag_14; rollmean_7, rollmean_14\\n\"\n",
        "        \"- dow_name, month_name (for one-hots)\"\n",
        "    )\n",
        "\n",
        "elif section == \"2. Data Overview\":\n",
        "    st.subheader(\"Quick Metrics & Trend\")\n",
        "    if len(df_daily):\n",
        "        dr_min, dr_max = df_daily.index.min(), df_daily.index.max()\n",
        "        dmin = dr_min.to_pydatetime().date(); dmax = dr_max.to_pydatetime().date()\n",
        "        rng = st.slider(\"Pick date range\", min_value=dmin, max_value=dmax, value=(dmin, dmax), format=\"YYYY-MM-DD\", key=\"ovr_rng\")\n",
        "        mask = (df_daily.index >= pd.to_datetime(rng[0])) & (df_daily.index <= pd.to_datetime(rng[1]))\n",
        "        st.plotly_chart(px.line(df_daily.loc[mask].reset_index(), x=\"date\", y=\"pax\", template=plot_theme, title=\"Daily Throughput\"), use_container_width=True)\n",
        "        st.subheader(\"Top Correlations with pax\")\n",
        "        cols = [c for c in [\"dow\",\"dom\",\"month\",\"is_weekend\",\"lag_1\",\"lag_7\",\"lag_14\",\"rollmean_7\",\"rollmean_14\"] if c in df_daily.columns]\n",
        "        corr = df_daily[[\"pax\"] + cols].corr().loc[cols, \"pax\"].sort_values(key=lambda s: s.abs(), ascending=False)\n",
        "        st.dataframe(corr.to_frame(\"corr_with_pax\"))\n",
        "    else:\n",
        "        st.info(\"No data yet. Upload the CSV in the sidebar.\")\n",
        "\n",
        "elif section == \"3. Exploratory Data Analysis\":\n",
        "    st.subheader(\"Preview & Summary\")\n",
        "    if granularity == \"Hourly\":\n",
        "        st.write(df_hourly.head()); st.caption(f\"Rows: {len(df_hourly):,} | Columns: {len(df_hourly.columns)}\")\n",
        "    else:\n",
        "        st.write(df_daily.head()); st.caption(f\"Rows: {len(df_daily):,} | Columns: {len(df_daily.columns)}\")\n",
        "    st.write((df_hourly if granularity==\"Hourly\" else df_daily)[[\"pax\"]].describe())\n",
        "\n",
        "    if len(df_daily):\n",
        "        st.subheader(\"Daily Throughput\")\n",
        "        dmin = df_daily.index.min().to_pydatetime().date(); dmax = df_daily.index.max().to_pydatetime().date()\n",
        "        rng = st.slider(\"Pick date range\", min_value=dmin, max_value=dmax, value=(dmin, dmax), format=\"YYYY-MM-DD\", key=\"eda_rng\")\n",
        "        mask = (df_daily.index >= pd.to_datetime(rng[0])) & (df_daily.index <= pd.to_datetime(rng[1]))\n",
        "        st.plotly_chart(px.line(df_daily.loc[mask].reset_index(), x=\"date\", y=\"pax\", template=plot_theme, title=\"Daily Pax\"), use_container_width=True)\n",
        "\n",
        "        c1, c2 = st.columns(2)\n",
        "        with c1:\n",
        "            st.subheader(\"Histogram\")\n",
        "            group = st.selectbox(\"Color by\", [\"day_of_week\",\"month_name\"], key=\"hist_group\")\n",
        "            src = df_hourly if granularity==\"Hourly\" else df_daily.reset_index()\n",
        "            st.plotly_chart(px.histogram(src, x=\"pax\", color=group, nbins=40, template=plot_theme), use_container_width=True)\n",
        "        with c2:\n",
        "            st.subheader(\"Boxplot\")\n",
        "            grp = st.selectbox(\"Group by\", [\"day_of_week\",\"month_name\"], key=\"box_group\")\n",
        "            src = df_hourly if granularity==\"Hourly\" else df_daily.reset_index()\n",
        "            st.plotly_chart(px.box(src, x=grp, y=\"pax\", template=plot_theme), use_container_width=True)\n",
        "\n",
        "        st.subheader(\"Hourly Heatmap by Day of Week\")\n",
        "        if granularity == \"Hourly\":\n",
        "            pivot = df_hourly.pivot_table(index=\"day_of_week\", columns=\"hour\", values=\"pax\", aggfunc=\"mean\")\n",
        "            days = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
        "            st.plotly_chart(px.imshow(pivot.reindex(days), aspect=\"auto\", color_continuous_scale=\"Viridis\", template=plot_theme), use_container_width=True)\n",
        "        else:\n",
        "            pivot = df_daily.groupby(\"day_of_week\")[\"pax\"].mean().reindex([\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"])\n",
        "            st.plotly_chart(px.imshow(pivot.to_frame().T, aspect=\"auto\", color_continuous_scale=\"Viridis\", template=plot_theme), use_container_width=True)\n",
        "\n",
        "        st.subheader(\"Correlation (Daily Features)\")\n",
        "        corr_cols = [c for c in [\"pax\",\"dow\",\"dom\",\"month\",\"is_weekend\",\"lag_1\",\"lag_7\",\"lag_14\",\"rollmean_7\",\"rollmean_14\"] if c in df_daily.columns]\n",
        "        corr = df_daily[corr_cols].corr()\n",
        "        st.plotly_chart(px.imshow(corr, text_auto=True, color_continuous_scale=\"RdBu_r\", origin=\"lower\", template=plot_theme), use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"No data yet. Upload the CSV in the sidebar.\")\n",
        "\n",
        "elif section == \"4. Feature Engineering\":\n",
        "    st.subheader(\"What was engineered and why\")\n",
        "    st.markdown(\n",
        "        \"- Calendar: `dow`, `dom`, `month`, `is_weekend`, plus names for one-hot encodings\\n\"\n",
        "        \"- Lags: `lag_1`, `lag_7`, `lag_14` (yesterday / weekly / fortnight)\\n\"\n",
        "        \"- Rolling means: `rollmean_7`, `rollmean_14` (smoothed baseline)\"\n",
        "    )\n",
        "    if len(df_daily):\n",
        "        st.subheader(\"Feature–Target Relationship\")\n",
        "        cols = [c for c in [\"pax\",\"dow\",\"dom\",\"month\",\"is_weekend\",\"lag_1\",\"lag_7\",\"lag_14\",\"rollmean_7\",\"rollmean_14\"] if c in df_daily.columns]\n",
        "        corr = df_daily[cols].corr().loc[[\"pax\"]].T.sort_values(\"pax\", key=np.abs, ascending=False)\n",
        "        st.dataframe(corr.rename(columns={\"pax\":\"corr_with_pax\"}))\n",
        "\n",
        "        c1, c2 = st.columns(2)\n",
        "        with c1:\n",
        "            if \"day_of_week\" in df_daily.columns:\n",
        "                st.plotly_chart(px.bar(df_daily.groupby(\"day_of_week\")[\"pax\"].mean().reindex(\n",
        "                    [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]).reset_index(),\n",
        "                    x=\"day_of_week\", y=\"pax\", template=plot_theme, title=\"Avg pax by weekday\"), use_container_width=True)\n",
        "        with c2:\n",
        "            if \"month_name\" in df_daily.columns:\n",
        "                order = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
        "                st.plotly_chart(px.bar(df_daily.groupby(\"month_name\")[\"pax\"].mean().reindex(order).reset_index(),\n",
        "                    x=\"month_name\", y=\"pax\", template=plot_theme, title=\"Avg pax by month\"), use_container_width=True)\n",
        "\n",
        "        st.subheader(\"Feature importance (Random Forest on train, log target)\")\n",
        "        if ml:\n",
        "            Xtr, ytr = ml[\"X_train_pp\"], ml[\"y_reg_train\"]\n",
        "            rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1).fit(Xtr, ytr)\n",
        "            importances = pd.Series(rf.feature_importances_, index=ml[\"feat_names\"]).sort_values(ascending=True).tail(20)\n",
        "            st.plotly_chart(px.bar(importances.reset_index(), x=\"index\", y=0, template=plot_theme, title=\"Top feature importances\").update_layout(xaxis_title=\"feature\", yaxis_title=\"importance\"), use_container_width=True)\n",
        "\n",
        "            st.subheader(\"Permutation importance (on test slice)\")\n",
        "            Xte, yte = ml[\"X_test_pp\"], ml[\"y_reg_test\"]\n",
        "            if len(yte) > 0:\n",
        "                take = min(len(yte), 300)\n",
        "                pi = permutation_importance(rf, Xte[:take], yte[:take], n_repeats=5, random_state=42, scoring=\"r2\")\n",
        "                pi_ser = pd.Series(pi.importances_mean, index=ml[\"feat_names\"]).sort_values(ascending=True).tail(20)\n",
        "                st.plotly_chart(px.bar(pi_ser.reset_index(), x=\"index\", y=0, template=plot_theme, title=\"Top permutation importances\").update_layout(xaxis_title=\"feature\", yaxis_title=\"Δscore\"), use_container_width=True)\n",
        "        else:\n",
        "            st.info(\"Not enough data to compute importances. Upload a CSV with at least ~30 days.\")\n",
        "    else:\n",
        "        st.info(\"No data yet. Upload the CSV in the sidebar.\")\n",
        "\n",
        "elif section == \"5. Predictive Modeling\":\n",
        "    if not ml:\n",
        "        st.info(\"Not enough data to train models. Upload a CSV with at least ~30 days.\")\n",
        "    else:\n",
        "        st.subheader(\"Regression (log1p target)\")\n",
        "        Xtr, Xte = ml[\"X_train_pp\"], ml[\"X_test_pp\"]\n",
        "        ytr, yte = ml[\"y_reg_train\"], ml[\"y_reg_test\"]\n",
        "\n",
        "        lin = LinearRegression().fit(Xtr, ytr); pred_lin = lin.predict(Xte)\n",
        "        dtr = DecisionTreeRegressor(max_depth=4, random_state=42).fit(Xtr, ytr); pred_dtr = dtr.predict(Xte)\n",
        "        rfr = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1).fit(Xtr, ytr); pred_rfr = rfr.predict(Xte)\n",
        "\n",
        "        def reg_eval(y_true_log, y_pred_log):\n",
        "            y_true = np.expm1(y_true_log); y_pred = np.expm1(y_pred_log)\n",
        "            return dict(R2_log=float(r2_score(y_true_log, y_pred_log)),\n",
        "                        RMSE=float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "                        MAE=float(mean_absolute_error(y_true, y_pred)))\n",
        "        reg_scores = {\n",
        "            \"LinearRegression\": reg_eval(yte, pred_lin),\n",
        "            \"DecisionTree(max_depth=4)\": reg_eval(yte, pred_dtr),\n",
        "            \"RandomForest(n=200)\": reg_eval(yte, pred_rfr)\n",
        "        }\n",
        "        st.dataframe(pd.DataFrame(reg_scores).T)\n",
        "\n",
        "        best_name = min(reg_scores, key=lambda k: reg_scores[k][\"MAE\"])\n",
        "        best_pred = {\"LinearRegression\": pred_lin, \"DecisionTree(max_depth=4)\": pred_dtr, \"RandomForest(n=200)\": pred_rfr}[best_name]\n",
        "        y_true = np.expm1(yte); y_hat = np.expm1(best_pred)\n",
        "        st.plotly_chart(px.line(pd.DataFrame({\"date\": ml[\"y_raw_test\"].index, \"Actual\": y_true, \"Pred\": y_hat}), x=\"date\", y=[\"Actual\",\"Pred\"], template=plot_theme, title=f\"Regression — Test ({best_name})\"), use_container_width=True)\n",
        "\n",
        "        st.subheader(\"Coefficients (Linear Regression)\")\n",
        "        coefs = pd.Series(lin.coef_, index=ml[\"feat_names\"]).sort_values(key=lambda s: s.abs(), ascending=False)\n",
        "        st.dataframe(coefs.rename(\"coef\").head(20))\n",
        "\n",
        "        st.divider()\n",
        "        st.subheader(\"Classification (surge vs normal)\")\n",
        "        ytr_c, yte_c = ml[\"y_cls_train\"], ml[\"y_cls_test\"]\n",
        "\n",
        "        logit = LogisticRegression(max_iter=1000, class_weight='balanced').fit(Xtr, ytr_c)\n",
        "        p_log = logit.predict_proba(Xte)[:,1]; y_log = (p_log >= 0.5).astype(int)\n",
        "\n",
        "        best_d, best_f1 = 0, -1\n",
        "        for d in [2,3,4,5]:\n",
        "            tmp = DecisionTreeClassifier(max_depth=d, random_state=42, class_weight='balanced').fit(Xtr, ytr_c)\n",
        "            f1 = f1_score(yte_c, tmp.predict(Xte), zero_division=0)\n",
        "            if f1 > best_f1: best_f1, best_d = f1, d\n",
        "        dtc = DecisionTreeClassifier(max_depth=best_d, random_state=42, class_weight='balanced').fit(Xtr, ytr_c)\n",
        "        p_dt = dtc.predict_proba(Xte)[:,1]; y_dt = (p_dt >= 0.5).astype(int)\n",
        "\n",
        "        rfc = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight='balanced').fit(Xtr, ytr_c)\n",
        "        p_rf = rfc.predict_proba(Xte)[:,1]; y_rf = (p_rf >= 0.5).astype(int)\n",
        "\n",
        "        def cls_eval(y_true, y_pred, y_prob):\n",
        "            return dict(Accuracy=float(accuracy_score(y_true,y_pred)),\n",
        "                        Precision=float(precision_score(y_true,y_pred,zero_division=0)),\n",
        "                        Recall=float(recall_score(y_true,y_pred,zero_division=0)),\n",
        "                        F1=float(f1_score(y_true,y_pred,zero_division=0)),\n",
        "                        ROC_AUC=float(roc_auc_score(y_true,y_prob)) if len(np.unique(y_true))>1 else np.nan)\n",
        "\n",
        "        cls_scores = {\n",
        "            \"Logistic(bal)\": cls_eval(yte_c, y_log, p_log),\n",
        "            f\"DecisionTree(depth={best_d})\": cls_eval(yte_c, y_dt, p_dt),\n",
        "            \"RandomForest(200,bal)\": cls_eval(yte_c, y_rf, p_rf)\n",
        "        }\n",
        "        st.dataframe(pd.DataFrame(cls_scores).T)\n",
        "\n",
        "        st.subheader(\"Logistic Regression — Top log-odds (drivers of surge=1)\")\n",
        "        log_coefs = pd.Series(logit.coef_[0], index=ml[\"feat_names\"]).sort_values()\n",
        "        st.dataframe(pd.concat([log_coefs.tail(10).rename(\"positive\"), log_coefs.head(10).rename(\"negative\")], axis=1))\n",
        "\n",
        "        names = list(cls_scores.keys())\n",
        "        best_cls = max(names, key=lambda n: (cls_scores[n][\"F1\"], cls_scores[n][\"ROC_AUC\"]))\n",
        "        best_pred = {\"Logistic(bal)\": y_log, f\"DecisionTree(depth={best_d})\": y_dt, \"RandomForest(200,bal)\": y_rf}[best_cls]\n",
        "        cm = confusion_matrix(ml[\"y_cls_test\"], best_pred)\n",
        "        st.plotly_chart(px.imshow(cm, text_auto=True, color_continuous_scale=\"Blues\", origin=\"lower\", labels=dict(x=\"Pred\", y=\"Actual\", color=\"Count\"), title=f\"Confusion Matrix — {best_cls}\", template=plot_theme), use_container_width=True)\n",
        "\n",
        "elif section == \"6. Model Selection & Tuning\":\n",
        "    if not ml:\n",
        "        st.info(\"Not enough data to tune models. Upload a CSV with at least ~30 days.\")\n",
        "    else:\n",
        "        st.subheader(\"Random Forest Classifier — Fast Randomized Search\")\n",
        "        Xtr, Xte = ml[\"X_train_pp\"], ml[\"X_test_pp\"]\n",
        "        ytr, yte = ml[\"y_cls_train\"], ml[\"y_cls_test\"]\n",
        "\n",
        "        tscv = TimeSeriesSplit(n_splits=3)\n",
        "        rf_est = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
        "        rf_space = {\n",
        "            \"n_estimators\": [100, 150, 200],\n",
        "            \"max_depth\": [None, 6, 10, 14],\n",
        "            \"min_samples_split\": [2, 5, 10],\n",
        "            \"min_samples_leaf\": [1, 2, 4],\n",
        "            \"max_features\": [\"sqrt\", \"log2\", 0.5]\n",
        "        }\n",
        "        rs = RandomizedSearchCV(rf_est, rf_space, n_iter=15, cv=tscv, scoring=\"f1\", random_state=42, n_jobs=-1, refit=True)\n",
        "        rs.fit(Xtr, ytr)\n",
        "\n",
        "        st.subheader(\"Best params\")\n",
        "        st.json(rs.best_params_)\n",
        "        st.subheader(\"Best CV F1\")\n",
        "        st.write(round(rs.best_score_, 4))\n",
        "\n",
        "        best = rs.best_estimator_.fit(Xtr, ytr)\n",
        "        prob = best.predict_proba(Xte)[:,1]; pred = (prob >= 0.5).astype(int)\n",
        "        test_scores = dict(\n",
        "            Accuracy=float(accuracy_score(yte, pred)),\n",
        "            Precision=float(precision_score(yte, pred, zero_division=0)),\n",
        "            Recall=float(recall_score(yte, pred, zero_division=0)),\n",
        "            F1=float(f1_score(yte, pred, zero_division=0)),\n",
        "            ROC_AUC=float(roc_auc_score(yte, prob)) if len(np.unique(yte))>1 else np.nan\n",
        "        )\n",
        "        st.subheader(\"Test metrics\")\n",
        "        st.json(test_scores)\n",
        "\n",
        "        cm = confusion_matrix(yte, pred)\n",
        "        st.plotly_chart(px.imshow(cm, text_auto=True, color_continuous_scale=\"Blues\", origin=\"lower\",\n",
        "                                  labels=dict(x=\"Pred\", y=\"Actual\", color=\"Count\"),\n",
        "                                  title=\"Confusion Matrix — Tuned RF\", template=plot_theme), use_container_width=True)\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(yte, prob)\n",
        "        st.plotly_chart(px.area(x=fpr, y=tpr, title=\"ROC Curve — Tuned RF\",\n",
        "                                labels=dict(x=\"FPR\", y=\"TPR\"), template=plot_theme).update_traces(mode=\"lines\"), use_container_width=True)\n",
        "\n",
        "elif section == \"7. Results & Conclusion\":\n",
        "    st.subheader(\"What the data showed\")\n",
        "    if len(df_daily):\n",
        "        bullets = []\n",
        "        if \"lag_7\" in df_daily.columns:\n",
        "            bullets.append(f\"- Weekly seasonality was strong (corr with lag_7 ≈ {df_daily['pax'].corr(df_daily['lag_7']):.2f}).\")\n",
        "        if \"is_weekend\" in df_daily.columns:\n",
        "            wk = df_daily.groupby(\"is_weekend\")[\"pax\"].mean()\n",
        "            if len(wk)==2:\n",
        "                bullets.append(f\"- Weekend mean ≈ {int(wk[1]):,} pax vs weekday mean ≈ {int(wk[0]):,} pax.\")\n",
        "        if bullets:\n",
        "            st.write(\"\\n\".join(bullets))\n",
        "    else:\n",
        "        st.write(\"Patterns were summarized in EDA.\")\n",
        "\n",
        "    st.subheader(\"Model outcomes \")\n",
        "    st.markdown(\n",
        "        \"- Regression: after the log transform, the Random Forest typically minimized MAE versus Linear/Tree and tracked the weekly wave reliably.\\n\"\n",
        "        \"- Classification: Logistic Regression provided clear drivers via log-odds; the Tree/Forest often delivered higher F1 for surge detection.\\n\"\n",
        "        \"- Consistent drivers: weekly lag and rolling means dominated; weekend flag and month dummies added smaller but stable shifts.\"\n",
        "    )\n",
        "\n",
        "\n",
        "'''\n",
        "with open(APP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(APP_CODE)\n",
        "\n",
        "# closed leftovers and restarted cleanly\n",
        "for name in (\"streamlit\", \"cloudflared\"):\n",
        "    try:\n",
        "        subprocess.run([\"pkill\", \"-f\", name], check=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# started Streamlit\n",
        "st_proc = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", APP_PATH, \"--server.port\", \"8501\", \"--server.address\", \"0.0.0.0\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1,\n",
        ")\n",
        "\n",
        "# gave Streamlit time to boot\n",
        "time.sleep(3)\n",
        "\n",
        "# opened Cloudflared tunnel and printed the public URL\n",
        "cf_proc = subprocess.Popen(\n",
        "    [CF_PATH, \"tunnel\", \"--url\", \"http://localhost:8501\", \"--no-autoupdate\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1,\n",
        ")\n",
        "\n",
        "public_url = None\n",
        "for _ in range(400):\n",
        "    line = cf_proc.stdout.readline()\n",
        "    if not line:\n",
        "        break\n",
        "    m = re.search(r\"https://[a-zA-Z0-9.-]+trycloudflare\\.com\", line)\n",
        "    if m:\n",
        "        public_url = m.group(0)\n",
        "        break\n",
        "\n",
        "print(\"\\n>>> Open this in a new browser tab:\", public_url if public_url else \"(no URL yet — re-run cell)\")"
      ]
    }
  ]
}